# -*- coding: utf-8 -*-
"""sh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lR5GLc3axcU0FWxpVd_pbpcfQhT3Gqg

import numpy as np
import matplotlib.pyplot as plt
import pandas
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

# making a fake data set here.
X, y = make_classification(n_features=20, n_redundant=0, n_informative=5, n_clusters_per_class=1)
X += 4.0 * np.random.uniform(size=X.shape)
myfakedataset = (X,y)

# Loading in CSV's using Pandas then formatting for use in algorithms
url = "steel_plates_fault.csv"
dataframe1 = pandas.read_csv(url)
steel_plates_fault_array = dataframe1.to_numpy()
X = steel_plates_fault_array[:, :-1] # for all but last column
y = steel_plates_fault_array[:, -1] # for last column
steel_plates_fault = (X,y)
url = "ionosphere.csv"
dataframe2 = pandas.read_csv(url)
ionosphere_array = dataframe2.to_numpy()
X = ionosphere_array[:, :-1] # for all but last column
y = ionosphere_array[:, -1] # for last column
ionosphere = (X,y)
url = "banknotes.csv"
dataframe3 = pandas.read_csv(url)
banknotes_array = dataframe3.to_numpy()
X = banknotes_array[:, :-1] # for all but last column
y = banknotes_array[:, -1] # for last column
banknotes = (X,y)


names = ["Nearest Neighbor", "Naive Bayes", "Decision Tree", "Logistic Regression", "Gradient Boosting",
         "Random Forest", "Neural Net"
        ]


datasets = [myfakedataset,
            steel_plates_fault,
            ionosphere,
            banknotes
           ]

classifiers = [KNeighborsClassifier,
               GaussianNB,
               DecisionTreeClassifier,
               LogisticRegression,
               GradientBoostingClassifier,
               RandomForestClassifier,
               MLPClassifier
              ]

clf_variable_names = ["n_neighbors =", "var_smoothing", "max_depth", "C", "max_depth", "max_depth", "alpha"]

clf_variables = [list(range(1, 6)), [1e-9,1e-5,1e-1], list(range(1, 11)), [.1,.5,1.0,2.0, 5.0],
                       list(range(1, 11)), list(range(1, 11)), [1e-5, 1e-3,0.1, 10.0]]


# accuraciesOfvar1 = [.9,.8,.85,etc]
# results = np.array([[accuraciesOfvar1], [accuraciesOfVar2], etc])

figure = plt.figure(figsize=(27, 9))
i = 1

# iterate over datasets
for ds_cnt, ds in enumerate(datasets):
    # preprocess dataset, split into training and test part
    X, y = ds
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = \
        train_test_split(X, y, test_size = 0.5, train_size = 0.5, random_state = 24)

    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # just plot the dataset first
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(['#FF0000', '#0000FF'])
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    if ds_cnt == 0:
        ax.set_title("Input data")
    # Plot the training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
               edgecolors='k')
    # Plot the testing points
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,
               edgecolors='k')
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    i += 1
    
    for clf, clfvn, list_of_variables in zip(classifiers, clf_variable_names, clf_variables):
        for variable in classifier_variables:
            clf(clfvn(variable))
            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
            clf.fit(X_train, y_train)
            score = clf.score(X_test, y_test)

